{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import datas.mnist_data as mnist_local\n",
    "from data import input_data\n",
    "mnist = input_data.read_data_sets(\"./data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore future waring\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 생성된 이미지들을 저장할 generated_outputs 폴더를 생성합니다.\n",
    "if not os.path.exists('generated_output/Autoencoder/'):\n",
    "    os.makedirs('generated_output/Autoencoder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 생성된 MNIST 이미지를 8x8 Grid로 보여주는 plot 함수를 정의합니다.\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(sample.reshape(28, 28))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 트레이닝 파라메터\n",
    "learning_rate = 0.01\n",
    "num_steps = 30000\n",
    "batch_size = 256\n",
    "\n",
    "display_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 네트워크 파라메터\n",
    "num_hidden_1 = 256 # 1st layer num features\n",
    "num_hidden_2 = 128 # 2nd layer num features (the latent dim)\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X(이미지)의 입력값(No labels, only pictures)\n",
    "X = tf.placeholder(np.float32, [None, num_input]) # 무한대 x 784 행렬\n",
    "z = tf.placeholder(tf.float32, [None, num_hidden_2])     # 인풋 Latent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델의 wright와 bias의 배열값\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 인코더 설정\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 디코더 설정\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 예측값(디코더에서의 출력값)\n",
    "y_pred = decoder_op\n",
    "\n",
    "# 원래값(인코더로의 입력값)\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss Function 및 optimizer 설정\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전체 변수 초기화 선언\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# TF session 시작\n",
    "sess = tf.Session()\n",
    "\n",
    "# initializer 실행\n",
    "sess.run(init)\n",
    "\n",
    "# 학습 시작\n",
    "# 학습횟수(epoch = num_steps = 30000)\n",
    "\n",
    "num_img = 0\n",
    "for epoch in range(1, num_steps+1):    \n",
    "    # batch_size 만큼 다음 mini batch를 가져옴\n",
    "    X_images, _ = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "    # 로그\n",
    "    sess.run(optimizer, feed_dict={X: X_images})\n",
    "    l = sess.run(loss, feed_dict={X: X_images})\n",
    "    \n",
    "#     # 다른 표기법\n",
    "#     _, l = sess.run([optimizer, loss], feed_dict={X: X_images})\n",
    "\n",
    "    # 500번 반복할때마다 생성된 이미지를 저장합니다.\n",
    "    if epoch % 500 == 0:\n",
    "            n = 4\n",
    "            canvas_recon = np.empty((28 * n, 28 * n))\n",
    "\n",
    "            for i in range(n):\n",
    "                # MNIST test set\n",
    "                test_X, _ = mnist.test.next_batch(batch_size)\n",
    "                g = sess.run(decoder_op, feed_dict={X: test_X})\n",
    "\n",
    "                # 재생성된 이미지를 가져와서 출력\n",
    "                for j in range(n):\n",
    "                    # Draw the generated digits\n",
    "                    canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
    "\n",
    "            plt.figure(figsize=(n, n))\n",
    "            plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray_r\")\n",
    "            plt.savefig('generated_output/Autoencoder/%s_iter%d.png' % (str(num_img).zfill(3), epoch), bbox_inches='tight')\n",
    "            num_img += 1\n",
    "            plt.close()\n",
    "    \n",
    "    # Display logs per step\n",
    "    if epoch % display_step == 0 or epoch == 1:\n",
    "        print('epoch %i: Minibatch Loss: %f' % (epoch, l))\n",
    "\n",
    "print(\"학습완료! (loss : \" + str(l) + \")\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 테스트 시작\n",
    "\n",
    "n = 4\n",
    "canvas_orig = np.empty((28 * n, 28 * n))\n",
    "canvas_recon = np.empty((28 * n, 28 * n))\n",
    "\n",
    "for i in range(n):\n",
    "    # MNIST test set\n",
    "    test_X, _ = mnist.test.next_batch(batch_size)\n",
    "    \n",
    "    g = sess.run(decoder_op, feed_dict={X: test_X})\n",
    "\n",
    "    # 원본 이미지를 가져와서 출력\n",
    "    for j in range(n):\n",
    "        canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = test_X[j].reshape([28, 28])\n",
    "    # 재생성된 이미지를 가져와서 출력\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits\n",
    "        canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
    "\n",
    "# 테스트 결과 출력\n",
    "print(\"Original Images\")     \n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray_r\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstructed Images\")\n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
