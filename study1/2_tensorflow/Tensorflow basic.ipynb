{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 텐서플로우를 구성하는 가장 기초적인 데이터 단위\n",
    " - n 차원 배열의 집합 또는 n 차원 배열을 의미\n",
    " - rank: 텐서의 차원\n",
    " - 텐서의 표현은 `numpy` 배열을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. # a rank 0 tensor; a scalar with shape []\n",
    "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3],\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3],\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorFlow programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1) 연산 그래프(computational graph) 정의\n",
    "  - 연산그래프는 텐서플로우 작업을 순차적으로 정의(표현)한 것으로 노드와 에지를 갖는 그래프 형태를 갖음 \n",
    "  - 연산그래프의 노드에는 텐서를 입력값으로 받아 연산하는 작업들이 위치 : `tf.Operation`\n",
    "  - 연산그래프의 에지에는 노드에 정의된 연산간에 주고 받는 데이터 들을 표현(텐서들이 그래프 상에서 흐름.) `tf.Tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2) 연산 그래프를 실행\n",
    "  - 연산그래프의 실행은 `tf.Session` 객체,텐서플로우가 실행되는 환경을 만들어서 진행됨\n",
    "  - 연산그래프의 작업을 CPU, GPU에 배정하고 실행을 위한 메서드를 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [default 그래프에 정의하기]\n",
    " - 3개의 노드(2개: constant op, 1개 matmul op)\n",
    " - 특정 그래프 객체에 명시적으로 연산을 정의하지 않는한 모든 연산은 전역 default 그래프에 정의됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mat_a = tf.constant([[3.0, 3.0]], dtype=tf.float32)\n",
    "mat_b = tf.constant([[2.0],[2.0]], dtype=tf.float32)\n",
    "product = tf.matmul(mat_a, mat_b)\n",
    "\n",
    "print(tf.get_default_graph() is product.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [특정 그래프에 연산 정의하기]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_1 = tf.Graph()\n",
    "with g_1.as_default():\n",
    "    mat_a = tf.constant([[3.0, 3.0]], dtype=tf.float32)\n",
    "    mat_b = tf.constant([[2.0],[2.0]], dtype=tf.float32)\n",
    "    product = tf.matmul(mat_a, mat_b)\n",
    "    print(product.graph is g_1)\n",
    "\n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "    mat_a = tf.constant([[3.0, 3.0]], dtype=tf.float32)\n",
    "    mat_b = tf.constant([[2.0],[2.0]], dtype=tf.float32)\n",
    "    product = tf.matmul(mat_a, mat_b)\n",
    "    print(product.graph is g_2)\n",
    "# with tf.Graph().as_default() as g_2:\n",
    "#     mat_a = tf.constant([[3.0, 3.0]], dtype=tf.float32)\n",
    "#     mat_b = tf.constant([[2.0],[2.0]], dtype=tf.float32)\n",
    "#     product = tf.matmul(mat_a, mat_b)\n",
    "#     print(product.graph is g_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [그래프 실행하기]\n",
    " - session 객체의 run 매서드 호출\n",
    " - default 그래프에 정의한 3개의 작업이 실행 (graph=None)\n",
    " - 사용한 session 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=g_2)\n",
    "print(sess.run(product))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - session 컨텍스트 매니저 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g_2) as sess:\n",
    "    print(sess.run(product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 연산 자원 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g_2) as sess:\n",
    "    with tf.device('/gpu:0'):\n",
    "        print(sess.run(product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [인터렉티브한 이용]\n",
    "  - Tensor.eval(), Operation.run() 메서드 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "x = tf.Variable([1.0,2.0])\n",
    "a = tf.constant([3.0,3.0])\n",
    "x.initializer.run()\n",
    "sub = tf.subtract(x,a)\n",
    "print(sub.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow tf.constant, tf.Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 연산 그래프에 정의된 연산을 수행하기 위해 필요한 데이터 값을 입력 위한 수단 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3-1. tf.constant\n",
    " - 상수 텐서를 생성하는 작업으로, `tf.constant` 연산 정의시 제공한 초기값을 갖는 텐서를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3-2. tf.Variable\n",
    " - 텐서플로우 프로그램에서 연산의 결과가 공유되고, 상태를 유지해야하는 경우 사용 \n",
    "   - ex) 학습을 진행하면서 모델의 파라미터가 업데이트 되야하므로 모델의 파라미터를 변수로 표현\n",
    " - 변수 연산을 정의하기 위해 텐서를 초기값으로 부여, 초기값으로 제공한 텐서로 변수 type과 shape이 결정됨\n",
    " - 변수 연산이 정의되면 타입과 변수 type과 shape은 고정됨, 변수 값인 텐서를 assign 메서드로 변경\n",
    " - 연산을 실행하기 전, 그래프 상에 정의된 변수를 명시적으로 초기화하는 작업 필요\n",
    "   - 초기화 연산을 실행(`tf.global_variable_initializer()`), 변수 값이 저장된 파일에서 복구, `assign` 메서드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tf.Variable(0, name=\"counter\")\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(state))\n",
    "    for _ in range(3):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### [변수의 저장과 복구]\n",
    "  - 변수를 이름과 텐서 값을 매핑해놓은 바이너리 파일(`.ckpt`)에 저장 가능\n",
    "  - `tf.train.Saver()` 객체를 이용하여 그래프 전체 변수와 지정된 리스트 변수를 저장하고 복구\n",
    "  - 저장될 때 사용되는 변수 명은 `Variable.name`이 기본 값\n",
    "  - `tf.train.Saver()` 객체에 딕셔너리를 저장할 이름(key), 저장할 값(value)로 전달하여 저장시 사용할 이름을 변경하거나 변수를 선택적으로 저장 가능\n",
    "    - ex) `tf.train.Saver({\"saved_v\":v})`\n",
    "  - 전체 변수를 파일에서 복구 시 변수 초기화가 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some variables.\n",
    "import tensorflow as tf\n",
    "import os, shutil\n",
    "\n",
    "with tf.variable_scope(\"variable\", reuse=tf.AUTO_REUSE):\n",
    "    v1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\n",
    "    v2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n",
    "\n",
    "inc_v1 = v1.assign(v1+1)\n",
    "dec_v2 = v2.assign(v2-1)\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, and save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "  # Do some work with the model.\n",
    "  inc_v1.op.run()\n",
    "  dec_v2.op.run()\n",
    "  # Save the variables to disk.\n",
    "  shutil.rmtree(\"./tmp/ckpt\")\n",
    "  os.mkdir(\"./tmp/ckpt\")\n",
    "  save_path = saver.save(sess, \"./tmp/ckpt/model.ckpt\")\n",
    "  print(\"Model saved in path: %s\" % save_path)\n",
    "    \n",
    "    \n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "chkp.print_tensors_in_checkpoint_file(save_path, tensor_name='',  all_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create some variables.\n",
    "with tf.variable_scope(\"variable\", reuse=tf.AUTO_REUSE):\n",
    "    v1 = tf.get_variable(\"v1\", shape=[3])\n",
    "    v2 = tf.get_variable(\"v2\", shape=[5])\n",
    "    \n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  saver.restore(sess, \"./tmp/ckpt/model.ckpt\")\n",
    "  print(\"Model restored.\")\n",
    "  # Check the values of the variables\n",
    "  print(\"v1 : %s\" % v1.eval())\n",
    "  print(\"v2 : %s\" % v2.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### - tf.get_variable()\n",
    "  - 공유 변수를 손쉽게 구현하기 위해 `tf.Variable`를 직접호출하지 않고 생성된 변수를 가져오거나 존재하지 않을 시 새롭게 생성\n",
    "     - ex) 매우 깊은 층을 갖는 심층심경망 네트워크 구현시 각 층마다 변수를 정의하는 데 따른 불편함을 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer1\", reuse=tf.AUTO_REUSE):\n",
    "     weight = tf.get_variable(\n",
    "            \"weight\", \n",
    "            [1, 2, 3], \n",
    "            dtype=tf.int32, \n",
    "            initializer=tf.zeros_initializer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 그래프 상에 정의된 작업 하나 이상의 작업 결과 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = tf.constant([3.0])\n",
    "input2 = tf.constant([2.0])\n",
    "input3 = tf.constant([5.0])\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = tf.multiply(input1, intermed)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 실행 시점에 연산 그래프 상으로 텐서 값을 제공하는 매커니즘\n",
    " - tf.placeholder를 이용 텐서가 입력될 공간을 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x1 = tf.placeholder(tf.float32, shape=(4, 4), name='input1')\n",
    "x2 = tf.placeholder(tf.float32, shape=(4, 4), name='input2')\n",
    "y = tf.matmul(x1, x2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#   print(sess.run(y))  # ERROR: will fail because x was not fed.\n",
    "  arr = np.random.rand(4, 4)\n",
    "  print(sess.run(y, feed_dict={x1: arr, x2:arr}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization : Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 텐서플로우가 제공하는 텐서보드를 활용하여 연산 그래프를 시각화\n",
    " - 그래프 실행 후 연산 결과 시각화\n",
    " - `tf.summary.FileWriter()` 객체에 연산 그래프와 연산 결과 값을 저장 후 텐서보드로 시각화   \n",
    " - 벡터: `tf.summary.histogram()`, 스칼라: `tf.summary.scalar()`로 시각화할 연산 값 설정\n",
    " - 텐서보드 실행 => `tensorboard --logdir=\"./logs/xor_log\"` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name='x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='y-input')\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases2\", b2)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                           tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_log\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir='./logs/xor_log'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
